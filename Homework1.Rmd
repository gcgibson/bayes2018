---
title: "Homework 1"
output:
  html_document: default
  pdf_document: default
subtitle: Casey Gibson
header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Problem 1


$$p(\theta) = \frac{1}{21} \text{ for   }   \theta=0,0.05,0.1,...,1$$
$$Y | \theta \sim Binom(n,\theta)$$

$$P(Y = 5 | \theta =.05) = \binom{n}{5}\theta^5(1-\theta)^{n-5} = \binom{100}{5}.05^5(1-.05)^{100-5} \approx .180 $$

$$P(Y = 5 | \theta =.5) = \binom{n}{5}\theta^5(1-\theta)^{n-5} = \binom{100}{5}.5^5(1-.5)^{100-5} \approx 5.94 \text{ x } 10^{-23} $$
$$P(\theta =.05 | Y = 5) = \frac{P(Y=5|\theta =.05 ) P(\theta=.05)}{P(Y=5)}$$
If we denote $\Theta = \{0,1,2..,20\}$ then $\theta \in \frac{\Theta}{20}$


$$P(Y = 5)  = \sum_{\theta' \in \Theta} P(Y,\theta') = \sum_{\theta' \in \Theta} P(Y |\theta')P(\theta') = \sum_{i=0}^{20} P(Y = 5 | \theta' = \frac{i}{20})P(\theta'=\frac{i}{20})$$
```{r, echo=FALSE}
marginal_of_y <- 0
for (i in seq(0,20)){
  marginal_of_y <- marginal_of_y + dbinom(x=5,size=100,i/20)*1.0/21
}

```
$$=\sum_{i=0}^{20} \binom{100}{5}(\frac{i}{20})^5(1-(\frac{i}{20}))^{100-5}\frac{1}{21} \approx .0102$$

$$P(\theta =.05 | Y = 5) = \frac{ .1800178* \frac{1}{21}}{.0102393} \approx .837$$


$$P(\theta =.5 | Y = 5) = \frac{ 5.93914e^{-23} \frac{1}{21}}{.0102393} \approx 2.76 \ x \ 10^{-22}$$

### Problem 2


```{r, echo=FALSE,message=FALSE,warning=FALSE}
library(pander)

posterior4 <- function(theta,p_of_theta){
  posterior_vals <- c()
  Z <- 0 
  for (i in 1:length(theta)){
    posterior_vals <- c(posterior_vals, dbinom(5,100,theta[i])*p_of_theta[i])
    Z <- Z +  dbinom(5,100,theta[i])*p_of_theta[i]
  }
  
   
  return (posterior_vals/Z)
}


prior <- function(theta,p,theta_support) {
  
 
  ISINARRAY <- FALSE 
  for (theta_prime in theta_support){
    if (theta == theta_prime){
      ISINARRAY <- TRUE
    }
  }
  if (ISINARRAY){
    return (p)
  }
  else {
    return (0)
  }
}

likelihood <- function(theta){
  return (dbinom(5,100,theta))
}

get_normalizing_constant <- function(p,theta_support){
  Z <- 0 
  for (theta in theta_support){
    Z <- Z + likelihood(theta)*prior(theta,p,theta_support)
  }
  return (Z)
}

posterior <- function(theta,p,theta_support){
  return (likelihood(theta)*prior(theta,p,theta_support)/get_normalizing_constant(p,theta_support))  
}



theta_support <- seq(0,20)/20
y <- c()
for (i in seq(0,1,.05)){
  y <- c(y,posterior(i,1/21,theta_support))
}
plot(x=seq(0,1,.05),y=y, ylab = expression(paste(italic("p"),"(",theta,"|y)", sep="")),xlab=expression(theta),main="Posterior under prior 1",type="h",ylim = c(0,1),lwd = 5)


theta_support <- seq(10,20)/20
y <- c()
for (i in seq(0,1,.05)){
  y <- c(y,posterior(i,1/11,theta_support))
}

plot(x=seq(0,1,.05),y=y, ylab = expression(paste(italic("p"),"(",theta,"|y)", sep="")),xlab=expression(theta),main="Posterior under prior 2",type="h",ylim = c(0,1.1),lwd = 5)


theta_support <- seq(0,10)/20
y <- c()
for (i in seq(0,1,.05)){
  y <- c(y,posterior(i,1/11,theta_support))
}
plot(x=seq(0,1,.05),y=y, ylab = expression(paste(italic("p"),"(",theta,"|y)", sep="")),xlab=expression(theta),main="Posterior under prior 3",type="h",ylim = c(0,1),lwd = 5)

y <- posterior4(theta = seq(0,1,by=.05),p_of_theta =seq(0,1,by=.05)/sum(seq(0,1,by=.05)) )
plot(x=seq(0,1,.05),y=y, ylab = expression(paste(italic("p"),"(",theta,"|y)", sep="")),xlab=expression(theta),main="Posterior under prior 4",type="h",ylim = c(0,1),lwd = 5)


theta_support <- seq(0,20)/20
e_theta_1 <- sum(theta_support*posterior(theta_support,1/21,theta_support))


theta_support <- seq(10,20)/20
e_theta_2 <- sum(theta_support*posterior(theta_support,1/11,theta_support))


theta_support <- seq(0,10)/20
e_theta_3 <- sum(theta_support*posterior(theta_support,1/11,theta_support))



posterior4vals <- posterior4(theta = seq(0,1,by=.05),p_of_theta =seq(0,1,by=.05)/sum(seq(0,1,by=.05)) )
e_theta_4 <- sum(seq(0,1,by=.05)*posterior4vals)

m <- data.frame(signif(e_theta_1,3), signif(e_theta_2,3),signif(e_theta_3,3),signif(e_theta_4,3))
colnames(m) <- c("Expected Value Prior 1","Expected Value Prior 2","Expected Value Prior 3","Expected Value Prior 4")


knitr::kable(m)


```

Prior 2 clearly pulls the posterior towards $1$ because it forces all thetas less than $.5$ to have $0$ probability. Prior 3 yeilds essentially the same results, which makes sense because the likelihood is so skewed towards the range $0,.5$, so excluding $.5,1$ does not change the posterior much. Prior 4 pushes the posterior slightly upwards, which again makes sense because the prior assigns higher probabilities to higher values of theta, pushing the posterior towards $1$. 

### Problem 3

```{r, echo=FALSE,message=FALSE,warning=FALSE}
library(pander)
posterior4 <- function(theta,p_of_theta){
  posterior_vals <- c()
  Z <- 0 
  for (i in 1:length(theta)){
    posterior_vals <- c(posterior_vals, dbinom(50,1000,theta[i])*p_of_theta[i])
    Z <- Z +  dbinom(50,1000,theta[i])*p_of_theta[i]
  }
  
   
  return (posterior_vals/Z)
}


prior <- function(theta,p,theta_support) {
  if (length(p) == 1){
    if (theta  %in% theta_support){
      return (p)
    }
    else{
      return (0)
    }
  }
  else {
    return (theta)
  }
}

likelihood <- function(theta){
  return (dbinom(50,1000,theta))
}

get_normalizing_constant <- function(p,theta_support){
  Z <- 0 
  for (theta in theta_support){
    Z <- Z + likelihood(theta)*prior(theta,p,theta_support)
  }
  return (Z)
}

posterior <- function(theta,p,theta_support){
  return (likelihood(theta)*prior(theta,p,theta_support)/get_normalizing_constant(p,theta_support))  
}

theta_support <- seq(0,20)/20
y <- c()
for (i in seq(0,1,.05)){
  y <- c(y,posterior(i,1/21,theta_support))
}
plot(x=seq(0,1,.05),y=y, ylab = expression(paste(italic("p"),"(",theta,"|y)", sep="")),xlab=expression(theta),main="Posterior under prior 1",type="h",ylim=c(0,1),lwd = 5)


theta_support <- seq(10,20)/20
y <- c()
for (i in seq(0,1,.05)){
  y <- c(y,posterior(i,1/11,theta_support))
}
plot(x=seq(0,1,.05),y=y, ylab = expression(paste(italic("p"),"(",theta,"|y)", sep="")),xlab=expression(theta),main="Posterior under prior 2",type="h",ylim=c(0,1),lwd = 5)


theta_support <- seq(0,10)/20
y <- c()
for (i in seq(0,1,.05)){
  y <- c(y,posterior(i,1/11,theta_support))
}
plot(x=seq(0,1,.05),y=y, ylab = expression(paste(italic("p"),"(",theta,"|y)", sep="")),xlab=expression(theta),main="Posterior under prior 3",type="h",ylim=c(0,1),lwd = 5)

theta_support <- seq(0,20)/20
y <- c()
for (i in seq(0,1,.05)){
  y <- c(y,posterior(i,theta_support,theta_support))
}
plot(x=seq(0,1,.05),y=y, ylab = expression(paste(italic("p"),"(",theta,"|y)", sep="")),xlab=expression(theta),main="Posterior under prior 4",type="h",ylim=c(0,1),lwd = 5)


theta_support <- seq(0,20)/20
e_theta_1 <- sum(theta_support*posterior(theta_support,1/21,theta_support))


theta_support <- seq(10,20)/20
e_theta_2 <- sum(theta_support*posterior(theta_support,1/11,theta_support))


theta_support <- seq(0,10)/20
e_theta_3 <- sum(theta_support*posterior(theta_support,1/11,theta_support))

theta_support <- seq(0,20)/20
e_theta_4 <- sum(theta_support*posterior(theta_support,theta_support,theta_support))


posterior4vals <- posterior4(theta = seq(0,1,by=.05),p_of_theta =seq(0,1,by=.05)/sum(seq(0,1,by=.05)) )
e_theta_4 <- sum(seq(0,1,by=.05)*posterior4vals)


m <- data.frame(signif(e_theta_1,3), signif(e_theta_2,3),signif(e_theta_3,3),signif(e_theta_4,3))
colnames(m) <- c("Expected Value Prior 1","Expected Value Prior 2","Expected Value Prior 3","Expected Value Prior 4")


knitr::kable(m)
```



As we get more data the effect of the prior diminishes (which is called "swamping" I think?). This makes more sense, to me at least, if we consider the log posterior.


$$p(\theta |y ) \propto p(y | \theta)p(\theta)$$

taking logs we see

$$log \ p(\theta|y) \propto log \ p(y | \theta)  + log \ p(\theta)$$

$$log / p(\theta|y) \propto log \ \prod_{i=1}^nL(\theta | y) + log \ p(\theta)$$
where $L(\theta | y)$ is the likelihood function.

This becomes
$$log \ p(\theta|y) \propto \sum_{i=1}^n log \ L(\theta | y) + log \ p(\theta)$$
Now we can clearly see, as $n$ increases the effect of the $log \ p(\theta)$ dimishes. It is harder for me to see this clearly in the non-log space because we are dealing with multiplication of small numbers. 


